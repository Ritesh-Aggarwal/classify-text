{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas, xgboost, numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading https://files.pythonhosted.org/packages/77/63/a9fa76de8dffe7455304c4ed635be4aa9c0bacef6e0633d87d5f54530c5c/tensorflow-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (92.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 92.5MB 13kB/s eta 0:00:011    33% |██████████▋                     | 30.8MB 2.6MB/s eta 0:00:24    60% |███████████████████▌            | 56.4MB 688kB/s eta 0:00:53    81% |██████████████████████████      | 75.3MB 1.2MB/s eta 0:00:14    90% |████████████████████████████▉   | 83.3MB 598kB/s eta 0:00:16    92% |█████████████████████████████▊  | 86.0MB 863kB/s eta 0:00:08\n",
      "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/90/85/64c82949765cfb246bbdaf5aca2d55f400f792655927a017710a78445def/Keras_Applications-1.0.7-py2.py3-none-any.whl\n",
      "Collecting absl-py>=0.1.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/31/bc/ab68120d1d89ae23b694a55fe2aece2f91194313b71f9b05a80b32d3c24b/absl-py-0.7.0.tar.gz (96kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 2.4MB/s a 0:00:011\n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/96/ba/a4702cbb6a3a485239fbe9525443446203f00771af9ac000fa3ef2788201/wheel-0.33.1-py2.py3-none-any.whl\n",
      "Collecting numpy>=1.13.3 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting six>=1.10.0 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
      "Collecting astor>=0.6.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
      "Collecting protobuf>=3.6.1 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/60/ca38e967360212ddbb004141a70f5f6d47296e1fba37964d8ac6cb631921/protobuf-3.7.0-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.2MB 579kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/7b/3ee06856ec30d5136cd2002408df1d111fcff269f3691147dbf3b8dc0ba2/tensorboard-1.13.0-py3-none-any.whl (3.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.2MB 317kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
      "\u001b[K    100% |████████████████████████████████| 368kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting gast>=0.2.0 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting grpcio>=1.8.6 (from tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/f4/dc/5503d89e530988eb7a1aed337dcb456ef8150f7c06132233bd9e41ec0215/grpcio-1.19.0-cp36-cp36m-manylinux1_x86_64.whl (10.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 10.8MB 113kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/bf/0315ef6a9fd3fc2346e85b0ff1f5f83ca17073f2c31ac719ab2e4da0d4a3/Keras_Preprocessing-1.0.9-py2.py3-none-any.whl\n",
      "Collecting h5py (from keras-applications>=1.0.6->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting setuptools (from protobuf>=3.6.1->tensorflow)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/6a/4b2fcefd2ea0868810e92d519dacac1ddc64a2e53ba9e3422c3b62b378a6/setuptools-40.8.0-py2.py3-none-any.whl\n",
      "Collecting werkzeug>=0.11.15 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/20/c4/12e3e56473e52375aa29c4764e70d1b8f3efa6682bef8d0aae04fe335243/Werkzeug-0.14.1-py2.py3-none-any.whl (322kB)\n",
      "\u001b[K    100% |████████████████████████████████| 327kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.14.0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/6b/5600647404ba15545ec37d2f7f58844d690baf2f81f3a60b862e48f29287/Markdown-3.0.1-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 3.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 3.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow)\n",
      "  Downloading https://files.pythonhosted.org/packages/14/09/12fe9a14237a6b7e0ba3a8d6fcf254bf4b10ec56a0185f73d651145e9222/pbr-5.1.3-py2.py3-none-any.whl (107kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 2.8MB/s ta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: absl-py, termcolor, gast\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/saksham/.cache/pip/wheels/90/db/f8/2c3101f72ef1ad434e4662853174126ce30201a3e163dcbeca\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/saksham/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Running setup.py bdist_wheel for gast ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/saksham/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built absl-py termcolor gast\n",
      "Installing collected packages: six, numpy, h5py, keras-applications, absl-py, wheel, astor, setuptools, protobuf, termcolor, werkzeug, grpcio, markdown, tensorboard, pbr, mock, tensorflow-estimator, gast, keras-preprocessing, tensorflow\n",
      "Successfully installed absl-py-0.7.0 astor-0.7.1 gast-0.2.2 grpcio-1.19.0 h5py-2.9.0 keras-applications-1.0.7 keras-preprocessing-1.0.9 markdown-3.0.1 mock-2.0.0 numpy-1.16.2 pbr-5.1.3 protobuf-3.7.0 setuptools-40.8.0 six-1.12.0 tensorboard-1.13.0 tensorflow-1.13.1 tensorflow-estimator-1.13.0 termcolor-1.1.0 werkzeug-0.14.1 wheel-0.33.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the dataset\n",
    "data = open('corpus').read()\n",
    "labels, texts = [], []\n",
    "for i, line in enumerate(data.split(\"\\n\")):\n",
    "    content = line.split()\n",
    "    labels.append(content[0])\n",
    "    texts.append(\" \".join(content[1:]))\n",
    "\n",
    "# create a dataframe using texts and lables\n",
    "trainDF = pandas.DataFrame()\n",
    "trainDF['text'] = texts\n",
    "trainDF['label'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text       label\n",
      "0     Stuning even for the non-gamer: This sound tra...  __label__2\n",
      "1     The best soundtrack ever to anything.: I'm rea...  __label__2\n",
      "2     Amazing!: This soundtrack is my favorite music...  __label__2\n",
      "3     Excellent Soundtrack: I truly like this soundt...  __label__2\n",
      "4     Remember, Pull Your Jaw Off The Floor After He...  __label__2\n",
      "5     an absolute masterpiece: I am quite sure any o...  __label__2\n",
      "6     Buyer beware: This is a self-published book, a...  __label__1\n",
      "7     Glorious story: I loved Whisper of the wicked ...  __label__2\n",
      "8     A FIVE STAR BOOK: I just finished reading Whis...  __label__2\n",
      "9     Whispers of the Wicked Saints: This was a easy...  __label__2\n",
      "10    The Worst!: A complete waste of time. Typograp...  __label__1\n",
      "11    Great book: This was a great book,I just could...  __label__2\n",
      "12    Great Read: I thought this book was brilliant,...  __label__2\n",
      "13    Oh please: I guess you have to be a romance no...  __label__1\n",
      "14    Awful beyond belief!: I feel I have to write t...  __label__1\n",
      "15    Don't try to fool us with fake reviews.: It's ...  __label__1\n",
      "16    A romantic zen baseball comedy: When you hear ...  __label__2\n",
      "17    Fashionable Compression Stockings!: After I ha...  __label__2\n",
      "18    Jobst UltraSheer Thigh High: Excellent product...  __label__2\n",
      "19    sizes recomended in the size chart are not rea...  __label__1\n",
      "20    mens ultrasheer: This model may be ok for sede...  __label__1\n",
      "21    Delicious cookie mix: I thought it was funny t...  __label__2\n",
      "22    Another Abysmal Digital Copy: Rather than scra...  __label__1\n",
      "23    A fascinating insight into the life of modern ...  __label__2\n",
      "24    i liked this album more then i thought i would...  __label__2\n",
      "25    Problem with charging smaller AAAs: I have had...  __label__1\n",
      "26    Works, but not as advertised: I bought one of ...  __label__1\n",
      "27    Disappointed: I read the reviews,made my purch...  __label__1\n",
      "28    Oh dear: I was excited to find a book ostensib...  __label__1\n",
      "29    Based on the reviews here I bought one and I'm...  __label__2\n",
      "...                                                 ...         ...\n",
      "9970  beware: The product does give the surround sou...  __label__1\n",
      "9971  happy i only wasted money for 2 Music Experien...  __label__1\n",
      "9972  oh my goodness!: If this is a single release, ...  __label__1\n",
      "9973  The dummy \"FATS\" is hysterical!!!!: ALL I can ...  __label__2\n",
      "9974  Dummy Scared the Be-Jesus Out of Me: Oh, God, ...  __label__1\n",
      "9975  More Ham-O-Rama Theatrics From Sir Anthony: Wh...  __label__1\n",
      "9976  Take The Knife Up The Hill And Rent This Movie...  __label__2\n",
      "9977  MAGIC ADS WERE SCARY!: Though the movie was fr...  __label__2\n",
      "9978  Deliciously disturbing ....Highly Underestimat...  __label__2\n",
      "9979  Magic: If you like Anthony Hopkins, this is on...  __label__2\n",
      "9980  Magic, on Blu Ray, starrring Anthony Hopkins a...  __label__2\n",
      "9981  A ventriloquists nightmare: Magic is a timeles...  __label__2\n",
      "9982  great movie massacred by tape quality: One of ...  __label__1\n",
      "9983  Early Hopkins story still sends chills through...  __label__2\n",
      "9984  The Only Dummy Is The Writer: \"Magic\" poses th...  __label__1\n",
      "9985  \"He's NO Dummy. . .\": Viewing \"Magic\" is when ...  __label__2\n",
      "9986  Amazingly suspenseful psychological thriller: ...  __label__2\n",
      "9987  A truly great horror movie: I saw this film la...  __label__2\n",
      "9988  Frightening movie with superb acting by Sir Ho...  __label__2\n",
      "9989  classic: i got this for my dad. it is super cr...  __label__2\n",
      "9990  Psychological thriller!: This movie really sca...  __label__2\n",
      "9991  A little more money than what I expected to sp...  __label__2\n",
      "9992  \"The Silence of the Dummies\": This is overall ...  __label__1\n",
      "9993  Mauled again - killing bears to enrich himself...  __label__1\n",
      "9994  Sorry Jim: As a former realtor, Mr. Cole owes ...  __label__1\n",
      "9995  A revelation of life in small town America in ...  __label__2\n",
      "9996  Great biography of a very interesting journali...  __label__2\n",
      "9997  Interesting Subject; Poor Presentation: You'd ...  __label__1\n",
      "9998  Don't buy: The box looked used and it is obvio...  __label__1\n",
      "9999  Beautiful Pen and Fast Delivery.: The pen was ...  __label__2\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(trainDF['text'], trainDF['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543    __label__1\n",
      "6324    __label__2\n",
      "7951    __label__2\n",
      "7986    __label__2\n",
      "2066    __label__2\n",
      "1716    __label__2\n",
      "4311    __label__1\n",
      "5951    __label__2\n",
      "8159    __label__1\n",
      "1814    __label__2\n",
      "9898    __label__1\n",
      "5922    __label__2\n",
      "1582    __label__1\n",
      "2855    __label__2\n",
      "8710    __label__2\n",
      "5726    __label__1\n",
      "5734    __label__2\n",
      "8157    __label__2\n",
      "5665    __label__1\n",
      "5033    __label__2\n",
      "3027    __label__1\n",
      "1685    __label__1\n",
      "1988    __label__2\n",
      "8464    __label__2\n",
      "3110    __label__1\n",
      "4691    __label__2\n",
      "8347    __label__2\n",
      "4495    __label__1\n",
      "3962    __label__1\n",
      "828     __label__2\n",
      "           ...    \n",
      "8582    __label__2\n",
      "5744    __label__2\n",
      "324     __label__2\n",
      "5370    __label__1\n",
      "9633    __label__2\n",
      "3602    __label__2\n",
      "9955    __label__1\n",
      "9887    __label__1\n",
      "6072    __label__2\n",
      "6357    __label__1\n",
      "694     __label__1\n",
      "444     __label__1\n",
      "6391    __label__1\n",
      "5825    __label__1\n",
      "8032    __label__1\n",
      "9051    __label__2\n",
      "8689    __label__2\n",
      "3721    __label__2\n",
      "6428    __label__1\n",
      "5409    __label__1\n",
      "8255    __label__2\n",
      "4809    __label__2\n",
      "2364    __label__1\n",
      "3745    __label__2\n",
      "7722    __label__2\n",
      "2010    __label__2\n",
      "8310    __label__2\n",
      "7258    __label__1\n",
      "7513    __label__2\n",
      "3723    __label__1\n",
      "Name: label, Length: 2500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer object \n",
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "count_vect.fit(trainDF['text'])\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 26)\t1\n",
      "  (0, 710)\t5\n",
      "  (0, 989)\t1\n",
      "  (0, 1052)\t1\n",
      "  (0, 1601)\t1\n",
      "  (0, 1743)\t2\n",
      "  (0, 1750)\t1\n",
      "  (0, 2016)\t1\n",
      "  (0, 2017)\t1\n",
      "  (0, 2136)\t1\n",
      "  (0, 5094)\t1\n",
      "  (0, 5849)\t1\n",
      "  (0, 6480)\t2\n",
      "  (0, 6492)\t1\n",
      "  (0, 7469)\t1\n",
      "  (0, 7569)\t1\n",
      "  (0, 8143)\t1\n",
      "  (0, 8748)\t1\n",
      "  (0, 8899)\t1\n",
      "  (0, 9918)\t1\n",
      "  (0, 10713)\t1\n",
      "  (0, 11591)\t1\n",
      "  (0, 12908)\t1\n",
      "  (0, 13224)\t2\n",
      "  (0, 14030)\t1\n",
      "  :\t:\n",
      "  (7499, 25027)\t1\n",
      "  (7499, 26107)\t1\n",
      "  (7499, 27417)\t1\n",
      "  (7499, 27765)\t1\n",
      "  (7499, 28020)\t1\n",
      "  (7499, 28076)\t1\n",
      "  (7499, 28082)\t5\n",
      "  (7499, 28106)\t2\n",
      "  (7499, 28121)\t1\n",
      "  (7499, 28173)\t1\n",
      "  (7499, 28224)\t2\n",
      "  (7499, 28595)\t1\n",
      "  (7499, 28984)\t1\n",
      "  (7499, 29618)\t1\n",
      "  (7499, 29787)\t1\n",
      "  (7499, 29966)\t1\n",
      "  (7499, 30106)\t1\n",
      "  (7499, 30299)\t1\n",
      "  (7499, 30831)\t1\n",
      "  (7499, 30851)\t2\n",
      "  (7499, 30965)\t1\n",
      "  (7499, 30976)\t1\n",
      "  (7499, 31096)\t1\n",
      "  (7499, 31102)\t1\n",
      "  (7499, 31514)\t1\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(trainDF['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "# characters level tf-idf\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram_chars.fit(trainDF['text'])\n",
    "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
    "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saksham/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8656\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/saksham/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/saksham/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "7500/7500 [==============================] - 4s 584us/step - loss: 0.5259\n",
      "NN, Ngram Level TF IDF Vectors 0.502\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture(input_size):\n",
    "    # create input layer \n",
    "    input_layer = layers.Input((input_size, ), sparse=True)\n",
    "    \n",
    "    # create hidden layer\n",
    "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
    "    \n",
    "    # create output layer\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
    "\n",
    "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
    "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
    "    return classifier \n",
    "\n",
    "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
    "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
    "print (\"NN, Ngram Level TF IDF Vectors\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4537)\t0.2789661404512955\n",
      "  (0, 4196)\t0.1726723233871585\n",
      "  (0, 4106)\t0.2124571711287434\n",
      "  (0, 4101)\t0.13931350150767105\n",
      "  (0, 3977)\t0.20942041430555516\n",
      "  (0, 3974)\t0.39686672017404606\n",
      "  (0, 3485)\t0.2625990198548298\n",
      "  (0, 2785)\t0.08013728971859356\n",
      "  (0, 2553)\t0.19335001928649403\n",
      "  (0, 2524)\t0.1723991628577871\n",
      "  (0, 2283)\t0.1852552298785334\n",
      "  (0, 1762)\t0.16456634559447525\n",
      "  (0, 1484)\t0.2499613220959175\n",
      "  (0, 978)\t0.1389207931494489\n",
      "  (0, 455)\t0.25970840876948037\n",
      "  (0, 432)\t0.2043140011981171\n",
      "  (0, 220)\t0.23437627697616004\n",
      "  (0, 135)\t0.42031379373541894\n",
      "  (1, 4875)\t0.21631330463090784\n",
      "  (1, 4873)\t0.3481491322004817\n",
      "  (1, 4780)\t0.1390625217822411\n",
      "  (1, 4678)\t0.23936704004410636\n",
      "  (1, 4516)\t0.2291105170565627\n",
      "  (1, 3913)\t0.20605678164336422\n",
      "  (1, 3681)\t0.13256234183552104\n",
      "  :\t:\n",
      "  (7499, 4072)\t0.16837531471554526\n",
      "  (7499, 3985)\t0.14708012407399546\n",
      "  (7499, 3655)\t0.24056046291141128\n",
      "  (7499, 3539)\t0.15932084038561214\n",
      "  (7499, 3264)\t0.2206124780250366\n",
      "  (7499, 2841)\t0.14782990782845454\n",
      "  (7499, 2809)\t0.19243344418824387\n",
      "  (7499, 2749)\t0.2266905094336141\n",
      "  (7499, 2432)\t0.2365313980366306\n",
      "  (7499, 2428)\t0.24201632152961872\n",
      "  (7499, 1907)\t0.08911221674781158\n",
      "  (7499, 1865)\t0.12459616573093232\n",
      "  (7499, 1524)\t0.2206124780250366\n",
      "  (7499, 1486)\t0.17903209412452054\n",
      "  (7499, 1340)\t0.23408696450539926\n",
      "  (7499, 1238)\t0.10953024357784608\n",
      "  (7499, 1215)\t0.20809477627871656\n",
      "  (7499, 1194)\t0.11841549608364609\n",
      "  (7499, 1119)\t0.22864575669156872\n",
      "  (7499, 866)\t0.1502848716088949\n",
      "  (7499, 430)\t0.20595611027415608\n",
      "  (7499, 427)\t0.22395244736743822\n",
      "  (7499, 425)\t0.17144612325339342\n",
      "  (7499, 222)\t0.18023248094825425\n",
      "  (7499, 192)\t0.20865100827872426\n"
     ]
    }
   ],
   "source": [
    "#tfidf_vect_ngram.get_feature_names()\n",
    "#tfidf_vect_ngram.vocabulary_\n",
    "print(xtrain_tfidf_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
